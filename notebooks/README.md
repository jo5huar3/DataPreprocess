
| Notebook                                 | Purpose and Outputs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **`01_Cryptol_Augmentation_Meta.ipynb`** | Data augmentation by producing a file for each top level defition in an existing Cryptol file.  Files are generated by selecting each top-level definition as a target and computing its recursive dependency closure; for a given target, all referenced top-level definitions are identified and included. Building an AST recursively, until no unresolved references remain, results in multiple self-contained Cryptol files.                                                                                                                                             |
| **`02_Build_Datasets.ipynb`**            | Uses the metrics CSV or a list of filenames to build three dataset variants: **with comments**, **without comments** and **hybrid** (comments selectively retained). Internally it calls the `build_datasets_from_sources` API (see `src/preprocessing/dataset_builder.py`) which chunks large files according to a context‑window budget. Outputs include JSONL files (`dataset_with_comments.jsonl`, `dataset_without_comments.jsonl`, `dataset_hybrid.jsonl`) and optional Parquet files.                                                                                   |
| **`03_Pair_Prompts.ipynb`**              | Pairs each code chunk with an instruction/prompt. For every Cryptol/Saw function or property, it generates an instruction describing the task and produces a chat‐style record with system and user messages. The resulting JSONL (e.g. `data/sft_pairs.jsonl`) is ready for supervised fine‑tuning of language models.                                                                                                                                                                                                                                                        |
| **`04_Build_Eval_Suite.ipynb`**          | Builds an evaluation suite by sampling representative tasks from the cleaned dataset. It writes `evals.jsonl`, where each row includes a natural‑language task description, optional setup code, a list of test assertions, and a type flag indicating whether the task is a function or a property.                                                                                                                                                                                                                                                                           |
| **`05_Run_Evals.ipynb`**                 | Loads `evals.jsonl` and runs the evaluation suite against a model. It constructs prompts using `SYSTEM_PROMPT` and template strings and calls either a HuggingFace inference client or a user‑provided `generate_fn`. For each task it extracts the generated Cryptol code, executes it against the provided tests via a Cryptol server (see `src/eval/eval_suite.py`), and appends pass/fail information. The notebook writes a timestamped text report (e.g. `eval_results_2026‑01‑20_12‑34‑56.txt`) summarising each task, the generated code and whether all tests passed. |
| **`06_Analyse_Results.ipynb`**           | Parses the evaluation report(s) produced by notebook 5 and computes aggregate metrics. It summarises pass rates, groups tasks by category and may produce charts or tables illustrating model performance across different task types.                                                                                                                                                                                                                                                                                                                                         |
| **`Evals_Monolithic.ipynb`**             | A convenience wrapper that runs the full evaluation suite on the entire set of tasks. It produces a single comprehensive report similar to notebook 5 but without splitting tasks into subsets.                                                                                                                                                                                                                                                                                                                                                                                |
| **`Evals_Small.ipynb`**                  | A lighter version of the evaluation notebook that operates on a small subset of tasks. Useful for testing model configurations or verifying that the evaluation infrastructure works before running the full suite.                                                                                                                                                                                                                                                                                                                                                            |