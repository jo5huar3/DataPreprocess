{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import os, dotenv, yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.chdir(Path(config[\"pythonpath\"]).expanduser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from src.preprocessing.quality_process import compute_file_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "VERSION = config[\"version\"]\n",
    "VARIATION = \"nocomments\"\n",
    "\n",
    "DATA = f\"data/good_syntax/verified_{VARIATION}_{VERSION}.jsonl\"\n",
    "\n",
    "MODEL_ID = config[\"model_id\"]\n",
    "\n",
    "codellama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "if codellama_tokenizer.pad_token is None:\n",
    "    codellama_tokenizer.pad_token = codellama_tokenizer.eos_token\n",
    "\n",
    "# from quality_process import compute_file_metrics  # <-- uncomment and fix path\n",
    "\n",
    "results = []\n",
    "with open(DATA, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Scoring rows\"):\n",
    "        row = json.loads(line)\n",
    "        results.append(\n",
    "            compute_file_metrics(\n",
    "                row[\"filename\"],\n",
    "                row[\"content\"],\n",
    "                model_tokenizer=codellama_tokenizer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992526d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74aeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StarCoder-like thresholds (tune if needed)\n",
    "MAX_BYTES         = 200_000\n",
    "MAX_NONASCII      = 0.20\n",
    "ENC_MAX_RUN_CHARS = 1024\n",
    "ENC_MAX_FRACTION  = 0.50\n",
    "MAX_LINES_TOTAL   = 100_000\n",
    "MAX_LINE_AVG_LEN  = 100\n",
    "MAX_LINE_MAX_LEN  = 1_000\n",
    "MIN_TOKENS_LANG   = 40      # language-token gate (Cryptol tokenizer)\n",
    "MAX_TOKENS_LANG   = 10_000  # optional upper bound\n",
    "MIN_TOKENS_MODEL  = 40      # only if youâ€™ve populated num_tokens_model\n",
    "MAX_TOKENS_MODEL  = 1600\n",
    "MAX_HEXNUM_RATIO  = 0.20\n",
    "\n",
    "\n",
    "# --- exact dedup (keep first occurrence of each sha1) ---\n",
    "# mark duplicates (True means \"is duplicate\" => drop later)\n",
    "dup_mask = df.duplicated(subset=[\"sha1\"], keep=\"first\")\n",
    "\n",
    "# --- encoded data (StarCoder) ---\n",
    "enc_mask = (df[\"enc_max_run\"] > ENC_MAX_RUN_CHARS) | (df[\"enc_fraction\"] > ENC_MAX_FRACTION)\n",
    "\n",
    "# --- long-line filters (StarCoder) ---\n",
    "longline_mask = (\n",
    "    (df[\"lines\"] > MAX_LINES_TOTAL) |\n",
    "    (df[\"avg_line_len\"] > MAX_LINE_AVG_LEN) |\n",
    "    (df[\"max_line_len\"] > MAX_LINE_MAX_LEN)\n",
    ")\n",
    "\n",
    "# --- binary-like content ---\n",
    "binary_mask = df[\"binary_like\"].fillna(False)\n",
    "\n",
    "# --- non-ascii density ---\n",
    "nonascii_mask = df[\"non_ascii_ratio\"].fillna(0) > MAX_NONASCII\n",
    "\n",
    "# --- size guardrail (bytes) ---\n",
    "bytes_mask = df[\"bytes\"].fillna(0) > MAX_BYTES\n",
    "\n",
    "# --- language-token bounds ---\n",
    "lang_small_mask = df[\"num_tokens_model\"].fillna(0) < MIN_TOKENS_MODEL\n",
    "lang_large_mask = df[\"num_tokens_model\"].fillna(0) > MAX_TOKENS_MODEL\n",
    "\n",
    "# --- shingles exist (needed for Jaccard) ---\n",
    "no_shingles_mask = df[\"num_shingles\"].fillna(0) <= 0\n",
    "\n",
    "# --- numeric/hex blob concentration ---\n",
    "hexnum_mask = df[\"hexnum_ratio\"].fillna(0) > MAX_HEXNUM_RATIO\n",
    "\n",
    "# --- model-token gate (only apply where available) ---\n",
    "if \"num_tokens_model\" in df.columns:\n",
    "    model_small_mask = df[\"num_tokens_model\"].fillna(np.inf) < MIN_TOKENS_MODEL\n",
    "else:\n",
    "    model_small_mask = pd.Series(False, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all hard-drop reasons\n",
    "drop_mask = (\n",
    "    dup_mask |\n",
    "    enc_mask |\n",
    "    longline_mask |\n",
    "    binary_mask |\n",
    "    nonascii_mask |\n",
    "    bytes_mask |\n",
    "    lang_small_mask |\n",
    "    lang_large_mask |\n",
    "    no_shingles_mask |\n",
    "    hexnum_mask |\n",
    "    model_small_mask\n",
    ")\n",
    "\n",
    "# Optional: compute a human-readable fail reason (first rule that tripped)\n",
    "def first_reason(i):\n",
    "    if dup_mask.iat[i]:          return \"exact_duplicate\"\n",
    "    if enc_mask.iat[i]:          return \"encoded_data\"\n",
    "    if longline_mask.iat[i]:     return \"long_lines\"\n",
    "    if binary_mask.iat[i]:       return \"binary_like\"\n",
    "    if nonascii_mask.iat[i]:     return \"too_much_nonascii\"\n",
    "    if bytes_mask.iat[i]:        return \"too_large_bytes\"\n",
    "    if lang_small_mask.iat[i]:   return \"too_few_lang_tokens\"\n",
    "    if lang_large_mask.iat[i]:   return \"too_many_lang_tokens\"\n",
    "    if no_shingles_mask.iat[i]:  return \"no_shingles\"\n",
    "    if hexnum_mask.iat[i]:       return \"hexnum_blob\"\n",
    "    if model_small_mask.iat[i]:  return \"too_few_model_tokens\"\n",
    "    return \"ok\"\n",
    "\n",
    "df = df.copy()\n",
    "df[\"quality_ok\"] = ~drop_mask\n",
    "df[\"fail_reason\"] = [first_reason(i) for i in range(len(df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_cols = [\n",
    "    \"filename\", \"sha1\",\n",
    "    # size/lines\n",
    "    \"bytes\", \"lines\", \"avg_line_len\", \"max_line_len\",\n",
    "    # content/encoding\n",
    "    \"non_ascii_ratio\", \"binary_like\",\n",
    "    \"enc_total_matched\", \"enc_max_run\", \"enc_fraction\",\n",
    "    \"enc_hits_base64\", \"enc_hits_hexbytes\", \"enc_hits_unicode\",\n",
    "    # tokens/shingles\n",
    "    \"num_tokens_lang\", \"k_shingle\", \"num_shingles\", \"hexnum_ratio\",\n",
    "    # model tokens (optional)\n",
    "    \"num_tokens_model\",\n",
    "    # path heuristic & status\n",
    "    \"quality_ok\", \"fail_reason\",\n",
    "]\n",
    "\n",
    "candidate_df = df.loc[df[\"quality_ok\"], dedup_cols].reset_index(drop=True)\n",
    "similar_process_df = pd.read_json(DATA, lines=True)\n",
    "similar_process_df = similar_process_df[similar_process_df['filename'].isin(candidate_df['filename'])].reset_index(drop=True)\n",
    "\n",
    "put_back_path = Path(\"data/dropped/files_to_put_back.csv\")\n",
    "'''\n",
    "if put_back_path.exists():\n",
    "    put_back_set = pd.read_csv(put_back_path)\n",
    "    put_back_filenames = set(put_back_set[\"filename\"].dropna().tolist())\n",
    "else:\n",
    "    put_back_filenames = set()\n",
    "\n",
    "for fname in put_back_filenames:\n",
    "    if fname in df['filename'].values:\n",
    "        candidate_df = pd.concat([candidate_df, df[df['filename'] == fname][dedup_cols]], ignore_index=True)\n",
    "        candidate_df.loc[candidate_df['filename'] == fname, 'quality_ok'] = True\n",
    "        '''\n",
    "similar_process_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0156792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[summary] total:\", len(df))\n",
    "print(\"[summary] kept :\", int(df[\"quality_ok\"].sum()))\n",
    "print(\"[summary] dropped:\", int((~df[\"quality_ok\"]).sum()))\n",
    "print(\"[summary] drop reasons:\")\n",
    "print(df.loc[~df[\"quality_ok\"], \"fail_reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ceeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = df[df[\"quality_ok\"] == False].copy().reset_index(drop=True)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9846b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data_set = df[~df['fail_reason'].isin(['ok', 'exact_duplicate'])].copy().reset_index(drop=True)\n",
    "#out_path = Path(f\"data/dropped/review_files_{VARIATION}_{VERSION}.csv\")\n",
    "#out_path.parent.mkdir(parents=True, exist_ok=True)  # create dirs if missing\n",
    "#review_data_set.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec151e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.similiar_process import run_from_dataframe\n",
    "\n",
    "# candidate_df must have an absolute-path 'filename' column.\n",
    "df_files, df_pairs, similar_files = run_from_dataframe(\n",
    "    similar_process_df,\n",
    "    filename_col=\"filename\",\n",
    "    content_col=\"content\",\n",
    "    out_dir=\"minhash_outputs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.cluster_process import run_clustering\n",
    "\n",
    "# If you already have df_files/df_pairs in memory:\n",
    "df_keep, df_drop, df_clusters = run_clustering(\n",
    "    df_files=df_files,          # from similiar_process\n",
    "    df_pairs=df_pairs,          # from similiar_process\n",
    "    jaccard_keep_threshold=0.70,\n",
    "    out_dir=\"minhash_outputs\",\n",
    "    content_lookup=None,        # or {filename: raw_text} if you want text-derived penalties\n",
    "    save_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[df['filename'].isin(df_keep['filename'].tolist())].copy()\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'quality_ok'\n",
    "    ] = False\n",
    "df.loc[\n",
    "    df['filename'].isin(df_drop['filename'].tolist()),\n",
    "    'fail_reason'\n",
    "    ] = 'similiar_file_exists'\n",
    "\n",
    "dropped = df[~df['filename'].isin(dataset['filename'].tolist())].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b19640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped.to_csv(f\"data/dropped/{VARIATION}_dropped_files_{VERSION}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "verified_files = set(dataset[\"filename\"].unique())\n",
    "\n",
    "#all_files_df = pd.read_json(DATA1, lines=True)\n",
    "all_files_filtered_df = similar_process_df[similar_process_df[\"filename\"].isin(verified_files)].reset_index(drop=True)\n",
    "\n",
    "pattern = r\"/\\d{3}_testCase\"   # slash + exactly 3 digits + \"_testCase\"\n",
    "mask = all_files_filtered_df[\"filename\"].astype(str).str.contains(pattern, regex=True, na=False)\n",
    "\n",
    "test_case_df = all_files_filtered_df[mask].copy().reset_index(drop=True)\n",
    "all_files_filtered_df = all_files_filtered_df[~mask].reset_index(drop=True)\n",
    "\n",
    "mask = all_files_filtered_df[\"filename\"].astype(str).str.contains(r\"/tests/\", regex=True, na=False)\n",
    "all_files_filtered_df = all_files_filtered_df[~mask]\n",
    "\n",
    "out_path = Path(f\"data/clean_datasets/{VARIATION}_{VERSION}.jsonl\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)  # create dirs if missing\n",
    "all_files_filtered_df.to_json(out_path, lines=True, orient=\"records\")\n",
    "test_case_df.to_json(f\"data/clean_datasets/testCases_{VARIATION}_{VERSION}.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"data/{VARIATION}_file_metrics_{VERSION}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Automated_Reasoning_for_Cryptography",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
